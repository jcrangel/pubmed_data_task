{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pubmed_data_task.dataset_creator import create_pmid2details_ontologies\n",
    "from pubmed_data_task import config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_pmid2details_ontologies() missing 1 required positional argument: 'e_linker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26857/769019700.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpmid2details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_pmid2details_ontologies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUBMED_RAW_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print('saving file pmid2details.pickle')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# handle = open('pmid2details.pickle', 'wb')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# pickle.dump(pmid2details, handle, protocol=pickle.HIGHEST_PROTOCOL)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_pmid2details_ontologies() missing 1 required positional argument: 'e_linker'"
     ]
    }
   ],
   "source": [
    "pmid2details = create_pmid2details_ontologies(config.PUBMED_RAW_PATH)\n",
    "# print('saving file pmid2details.pickle')\n",
    "# handle = open('pmid2details.pickle', 'wb')\n",
    "# pickle.dump(pmid2details, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pubmed_parser as pp\n",
    "import torch.multiprocessing as tm\n",
    "from pubmed_data_task.dataset_creator import create_pmid2details_ontologies, load_EL_onto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = tm.Pool(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mention2pem dictionary ...\n",
      "Loading entity description dictionary ...\n",
      "NUmber of entities:  2680002\n",
      "Loading dictionary of term frequency ...\n",
      "Number of term in the collection:  3506008\n",
      "2021-12-13 16:43:52,233 loading file /home/julio/repos/dkou_linker/resources/taggers/sota-ner-flair/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "e_linker = load_EL_onto()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_article():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: /home/julio/repos/queryengine/data/pubmed_raw/pubmed21n0001.xml.gz\n",
      "[FINISHED]Parsing file: /home/julio/repos/queryengine/data/pubmed_raw/pubmed21n0001.xml.gz\n",
      "Linking articles:\n"
     ]
    }
   ],
   "source": [
    "data_path = config.PUBMED_RAW_PATH\n",
    "# files = glob.glob(\"../data/*.xml.gz\")\n",
    "# files = glob.glob(data_path+\"/*.xml.gz\")\n",
    "#DEBUG\n",
    "file = data_path+'/pubmed21n0001.xml.gz'\n",
    "\n",
    "\n",
    "\n",
    "print('Parsing file:', file)\n",
    "_articles = pp.parse_medline_xml(file)\n",
    "print('[FINISHED]Parsing file:', file)\n",
    "    # https: // stackoverflow.com/questions/5352546/extract-subset-of-key-value-pairs-from-python-dictionary-object\n",
    "texts = []\n",
    "for article in _articles:   \n",
    "    # pmid2details[int(article['pmid'])] = {\n",
    "    #     k: article[k] for k in fields}\n",
    "    pmid = int(article['pmid'])\n",
    "    title = article['title']\n",
    "    abstract = article['abstract']\n",
    "    text = title+abstract\n",
    "    texts.append(text)\n",
    "\n",
    "print('Linking articles:')\n",
    "\n",
    "spans = pool.map(e_linker.link_entities,texts )\n",
    "    # spans = e_linker.link_entities(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parallelize by process_file\n",
    "# Parallelize by creating several instances of e_linker\n",
    "# Improve entity linker...\n",
    "#       -filter by p(e|m) \n",
    "#       -Don't use NER BERT . Not good? since the purpose of the paper is to being able to use the\n",
    "#        ontologies files to create a NER tool. \n",
    "#       -Batch all the text first into the NER, then parallelize the entity linking next steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7111a28d93937e44de81b263355d7295881e69712709cd005f9d5c465abfe2b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('ontonerd': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
